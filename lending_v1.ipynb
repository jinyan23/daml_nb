{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stats models \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tasks\n",
    "1. Data understanding and preprocessing\n",
    "2. Create a train and test set (for subsequent machine learning)\n",
    "3. Data cleaning and feature engineering\n",
    "   * read up about for feature engineering\n",
    "4. Visualization\n",
    "5. Machine Learning\n",
    "\n",
    "##### Important Features to Notice\n",
    "`loan_status` = can exclude \"Current\" or \"In Grace Period\" since we cannot tell if the loan will be paid or defaulted\n",
    "`issue_d` = loan issue date (month)\n",
    "`term` = only 36 or 60 months, can treat as categorical data (can try changing and not changing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the 1.68 GB data file.\n",
    "df = pd.read_csv(\"../dataset/lendingclub/accepted_2007_to_2018Q4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data that are not numerical\n",
    "df.select_dtypes(include=[\"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last two rows of the data frame.\n",
    "df = df.iloc[:-2, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the rows and features to drop:\n",
    "1. Drop `loan_status` which are not `Fully Paid` or `Charged Off`\n",
    "2. Contain high percentage of missing values\n",
    "3. Multicollinearity\n",
    "4. Features not associated with loan status\n",
    "\n",
    "Since we're going to use this dataset to do prediction of whether a loan is fully paid or charged off/defaulted, we can drop the events where the loans are still in place: current and in grace period or late. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at cross tabulation between the feature term and target loan_status\n",
    "pd.crosstab(df[\"term\"], df[\"loan_status\"])\n",
    "\n",
    "# Subset data frame based on loan status - Charged Off and Fully Paid\n",
    "mapping = {\"Does not meet the credit policy. Status:Charged Off\": \"Charged Off\",\n",
    "           \"Default\": \"Charged Off\",\n",
    "           \"Does not meet the credit policy. Status:Fully Paid\": \"Fully Paid\"}\n",
    "\n",
    "df[\"loan_status\"] = df[\"loan_status\"].replace(mapping)\n",
    "\n",
    "df = df[~df[\"loan_status\"].isin([\"Current\", \n",
    "                                 \"In Grace Period\", \n",
    "                                 \"Late (16-30 days)\",\n",
    "                                 \"Late (31-120 days)\"])]\n",
    "\n",
    "pd.crosstab(df[\"term\"], df[\"loan_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the percentage of missing values from each feature.\n",
    "ms_values_count = df.isnull().sum()\n",
    "ms_values_perc = 100 * ms_values_count / len(df)\n",
    "\n",
    "ms_values_df = pd.DataFrame({\"ms_values_count\": ms_values_count,\n",
    "                             \"ms_values_perc\": ms_values_perc})\n",
    "ms_values_df.sort_values(\"ms_values_perc\", ascending=False, inplace=True)\n",
    "ms_values_df[ms_values_df[\"ms_values_perc\"] > 0]\n",
    "\n",
    "# Remove features with more than 50% missing values. \n",
    "feat_rm = list(ms_values_df[ms_values_df[\"ms_values_perc\"] > 50].index)\n",
    "df.drop(feat_rm, axis=1, inplace=True)\n",
    "\n",
    "# `id` should not be associated with loan status\n",
    "# `policy_code` only has one value\n",
    "# `out_prncp` and `out_prncp_inv` mainly just a single value\n",
    "df.drop([\"id\", \"policy_code\", \"out_prncp\", \"out_prncp_inv\"], axis=1, inplace=True)\n",
    "\n",
    "# Drop rows that contain missing values.\n",
    "df.dropna(axis=0, how=\"any\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical data, we can check if there are any highly correlated features using Pearson correlation, the default from `pd.DataFrame.corr()`. We can then remove features that are highly correlated (>0.8 on Pearson correlation coefficient). \n",
    "\n",
    "Variance Inflation Factor (VIF) gives you a measure of how much the variance of an estimated regression coefficient increases due to the presence of correlated features. \n",
    "\n",
    "* VIF > 5 --> the variance of estimated regression coefficient increases largely due to the presence of that feature. \n",
    "* VIF = 1 --> the variance of estimated regression coefficient does not increase despite the presence of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is already features that are one-hot encoded.\n",
    "for col in df.columns:\n",
    "    if set(df[col].unique()) == {0, 1}:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corrmat = df.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corrmat, xticklabels=True, yticklabels=True, cmap='RdYlGn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of tuples that contain the pair of highly correlated features.\n",
    "def find_corr_pairs(df, thresh=0.8):\n",
    "    corr_pairs = []\n",
    "    for row in corrmat.index:\n",
    "        for col in corrmat.columns:\n",
    "            if (row!=col) and (corrmat.loc[row, col] >= thresh):\n",
    "                corr_pairs.append((row, col))\n",
    "    return corr_pairs\n",
    "\n",
    "# Setting pearson correlation threshold as 0.8\n",
    "corr_pairs = find_corr_pairs(df, thresh=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only requires the numerical features.\n",
    "# Same number of features as used in the correlation matrix.\n",
    "df_num = df.select_dtypes(include=\"number\")\n",
    "\n",
    "# This takes a little while as there are >900,000 rows. \n",
    "# To calculate the VIF for each feature using a subset of the dataset. \n",
    "df_num_subset = df_num.sample(10000)\n",
    "\n",
    "for i, k in enumerate(df_num_subset.columns):\n",
    "    print(k, \": \", round(variance_inflation_factor(df_num_subset.values, i), 2), sep=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of features with very high VIF values. We shall set the threshold for VIF to be 5, and pick out the columns with VIF value more than the set threshold. \n",
    "\n",
    "The columns will then be matched against the correlation list to see if they match.\n",
    "\n",
    "The correlation matrix can be calculated again to see if there are any more highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df.select_dtypes(include=\"number\")\n",
    "\n",
    "def vif_drop(df, vif_thresh=5):\n",
    "    \n",
    "    dropped = True\n",
    "    feat_to_drop = []\n",
    "\n",
    "    # VIF is re-calculated each time a feature is dropped.\n",
    "    # Iteration completed when all VIF < 5.\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        \n",
    "        # Select a random subset of sample on each iteration to calculate the VIF\n",
    "        df_num_subset = df.sample(10000)\n",
    "\n",
    "        vif_list = []\n",
    "        for i, k in enumerate(df_num_subset.columns): \n",
    "            vif_list.append(round(variance_inflation_factor(df_num_subset.values, i), 3))\n",
    "        \n",
    "        # Match the VIF to the feature\n",
    "        vif_series = pd.Series(vif_list, index=df_num_subset.columns)\n",
    "        vif_series = vif_series.sort_values(ascending=False)\n",
    "        \n",
    "        if vif_series.iloc[0] > vif_thresh:\n",
    "            \n",
    "            # Save the features that are dropped\n",
    "            feat_to_drop.append(vif_series.index[0])\n",
    "            print(\"feature dropped: \", vif_series.index[0])\n",
    "            \n",
    "            # Drop the feature when the VIF > 5\n",
    "            df.drop(vif_series.index[0], axis=1, inplace=True)\n",
    "            dropped = True\n",
    "    \n",
    "    return feat_to_drop\n",
    "\n",
    "feat_to_drop = vif_drop(df_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a random forest model with just the numerical values to get an idea of what the accuracy score looks like now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with just numerical features.\n",
    "features = df.select_dtypes(include=\"number\").drop(feat_to_drop, axis=1)\n",
    "target = df[\"loan_status\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, \n",
    "                                                    target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1988)\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=features.columns)\n",
    "x_test = pd.DataFrame(x_test, columns=features.columns)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=1988)\n",
    "rf.fit(x_train, y_train);\n",
    "\n",
    "y_pred = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances plot\n",
    "feat_imp = pd.DataFrame()\n",
    "\n",
    "feat_imp['Features'] = x_train.columns.values\n",
    "feat_imp['importance'] = rf.feature_importances_\n",
    "\n",
    "feat_imp = feat_imp.sort_values(by='importance', ascending=False)\n",
    "sns.barplot(y=\"Features\", x=\"importance\", data=feat_imp);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top three features shown to be important with the random forest:\n",
    "1. `collection_recovery_fee` = post charge off collection fee\n",
    "2. `last_pymnt_amnt` = last total payment amount received\n",
    "3. `total_rec_int` = interest received to date\n",
    "\n",
    "These top two features don't actually seem useful in determining if a person is going to default or pay off his loan since those features will only happen at (or near) the end of the loan period. We can remove those two features and run dataset through the VIF again.\n",
    "\n",
    "We are also going to drop those features that are not associated with the loan status outcome at all, using a cut-off (arbitrary) of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Run\n",
    "c_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(c_matrix, \"\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features that are not associated with the loan status outcome.\n",
    "feat_to_drop_2 = [\"collection_recovery_fee\", \"last_pymnt_amnt\"]\n",
    "feat_to_drop_3 = list(feat_imp[feat_imp[\"importance\"] < 0.01][\"Features\"].values)\n",
    "\n",
    "# Run the VIF again\n",
    "df_num = (df.select_dtypes(include=\"number\")\n",
    "            .drop(feat_to_drop_2, axis=1)\n",
    "            .drop(feat_to_drop_3, axis=1))\n",
    "\n",
    "feat_to_drop = vif_drop(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Run with lesser columns\n",
    "features = (df.select_dtypes(include=\"number\")\n",
    "              .drop(feat_to_drop, axis=1)\n",
    "              .drop(feat_to_drop_2, axis=1)\n",
    "              .drop(feat_to_drop_3, axis=1))\n",
    "\n",
    "target = df[\"loan_status\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, \n",
    "                                                    target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1988)\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=features.columns)\n",
    "x_test = pd.DataFrame(x_test, columns=features.columns)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=1988)\n",
    "\n",
    "rf.fit(x_train, y_train);\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Evaluation Reports\n",
    "c_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(c_matrix, \"\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping of the `collection_recovery_fee` and `last_pymnt_amnt` decreases the recall score for the `Charged Off` target further, showing that those two features are indeed more associated with the `Charged Off` target. However, those two features cannot be used in the model as the information are only available at the end of the loan period. \n",
    "\n",
    "Apart from training models off the data, there must also be certain domain knowledge, or knowledge of the features to make the selection of features for model training quicker and more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Run with StandardScaler()\n",
    "# Try with StandardScaler for the numerical features\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, \n",
    "                                                    target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1988)\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=features.columns)\n",
    "x_test = pd.DataFrame(x_test, columns=features.columns)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=1988)\n",
    "\n",
    "rf.fit(x_train, y_train);\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Evaluation Reports\n",
    "c_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(c_matrix, \"\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial run with random forest classifier using just the numerical features, the accuracy of the model seems good at 0.94. All other metrics look decent except for the recall for `Charged Off` loan status. The recall metric of a classification problem is the ratio of number of true positive predictions over the number of actual positive cases. This means out of 100 cases of loan takers who defaulted on the payment, the model is only able to pick out 74 cases. For a loan company, it is not ideal as it means 26 applications for loans are from people who eventually default on their loans. These are considered losses for a loan company. \n",
    "\n",
    "The model must be able to identify people who will default on their loans based on the data available as accurately as possible. In contrast with the recall for `Fully Paid` loan status, it is not as crucial to identify people who eventually fully pay off their loans because these are not considered losses for the company. \n",
    "\n",
    "Since there are some features that will only be available at the end of the loan period, and there are too many features to sort through, the next version of machine learning for this lending dataset will include only the features more widely used by the community. Listed below. In addition, the categorical features will be included in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More widely used features\n",
    "usecols = [\"loan_amnt\", \"term\", \"int_rate\", \"installment\", \"grade\", \"sub_grade\", \"emp_title\",\n",
    "           \"emp_length\", \"home_ownership\", \"annual_inc\", \"verification_status\", \"issue_d\", \"loan_status\", \"purpose\",\n",
    "           \"title\", \"zip_code\", \"addr_state\", \"dti\", \"earliest_cr_line\", \"open_acc\", \"pub_rec\",\n",
    "           \"revol_bal\", \"revol_util\", \"total_acc\", \"initial_list_status\", \"application_type\", \"mort_acc\", \"pub_rec_bankruptcies\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (v3.10.7:6cc6b13308, Sep  5 2022, 14:02:52) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd11f3380fa70e3455920893a60e5b10d61731084a1d9c89e6b3d2bda2e7bc1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
