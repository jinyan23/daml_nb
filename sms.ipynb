{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Natural Language Processing (NLP) has gained significant attention in the field of artificial intelligence due to the rise of technologies like ChatGPT, making it one of the most prominent and dynamic areas of research and application today.\n",
    "\n",
    "NLP involves analyzing, understanding and manipulation of human language by computers to extract meaningful insights to perform sentiment analysis or text classification. Here, we shall employ NLP techniques in performing text classification. \n",
    "\n",
    "Text classification can be performed either through supervised and unsupervised learning. For supervised learning, the learning model has the list of targets (or labels) to check the predictions against. Whereas in unsupervised learning, the observations are just partitioned into groups based on similarity scores from the features alone. \n",
    "\n",
    "In this exercise, we aim to perform both supervised and unsupervised learning on a set of SMS text corpus that carries ham and spam text messages. The goal is to classify a text into ham or spam based solely on the words present in the text (or the text corpus).\n",
    "\n",
    "Link to dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset \n",
    "\n",
    "## Problem Statement\n",
    "Will we be able to classify text messages (SMS) into ham or spam based solely on the words in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "df = pd.read_csv(\"../dataset/nus_sms_corpus/spam.csv\", \n",
    "                 usecols=[0, 1],\n",
    "                 encoding=\"ISO-8859-1\")\n",
    "df = df.rename(columns={\"v1\":\"label\",\n",
    "                        \"v2\":\"text\"})\n",
    "\n",
    "sms = df[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows of data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [skip] Read in output from previous run\n",
    "df = pd.read_csv(\"../dataset/nus_sms_corpus/spam2.csv\")\n",
    "corpus_w_trigrams2 = df[\"trigrams_list\"].values\n",
    "\n",
    "# Subset out the corpus with trigrams and convert to list of lists\n",
    "import ast\n",
    "literal_w_trigrams = df[\"trigrams_list\"].apply(ast.literal_eval)\n",
    "corpus_w_trigrams = list(literal_w_trigrams.values)\n",
    "\n",
    "sms = df[\"text\"].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Text Formatting\n",
    "The dataset consists of only two columns: the targets (or labels) and the text. A number of things need to be done.\n",
    "1. Convert text encoding from ASCII to UTF-8.\n",
    "2. Convert text to lowercases. \n",
    "3. Remove punctuations.\n",
    "4. Remove stop words that very common and carry no meaning.\n",
    "5. Truncate all whitespaces between words.\n",
    "\n",
    "### Lemmatization\n",
    "Text lemmatization is the process of trimming words to their root base known as lemma. Lemmatization retains the meaning of the words truncated. This is done to reduce the total unique words within the corpus to make the NLP process more manageable. Lemmatization reduces the vocabulary size while maintain feature representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to format text\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "list_stopwords = stopwords.words(\"english\") + [\"I\", \"U\", \"u\", \"You\", \"ur\", \"2\", \"4\"]\n",
    "\n",
    "def format_text(text):\n",
    "\n",
    "    # Remove accents\n",
    "    rm_accent = normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    \n",
    "    # Remove punctuation from text    \n",
    "    rm_punc_var = re.sub(r\"[^\\w\\s]\", \"\", rm_accent)\n",
    "    rm_punc = re.sub(r\"_\", \"\", rm_punc_var)       \n",
    "    \n",
    "    # Remove frequently occuring words from text\n",
    "    rm_stopwords_list = [word.lower() for word in rm_punc.split() if word.lower() not in list_stopwords]\n",
    "    rm_stopwords = \" \".join(rm_stopwords_list)\n",
    "    \n",
    "    # Reduce all whitespaces between words to one\n",
    "    new_text = re.sub(\"\\s+\", \" \", rm_stopwords)\n",
    "    \n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to lemmatize text\n",
    "from nltk import WordNetLemmatizer, pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Instantialize the lemmatizer. \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    # Tokenize divides the string into a list of substrings\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    # Tag the tokenized substrings with the \"part of speech tag\".\n",
    "    tagged_token = pos_tag(tokenized_text)\n",
    "    \n",
    "    root = []\n",
    "    \n",
    "    for token in tagged_token:\n",
    "        \n",
    "        \n",
    "        word = token[0]\n",
    "        tag = token[1][0]\n",
    "        \n",
    "        \n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemmatizer.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemmatizer.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemmatizer.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemmatizer.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    \n",
    "    # Remove single character word\n",
    "    root = [word for word in root if len(word) > 1]\n",
    "    \n",
    "    return root             # return list of strings in a tokenized format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and lemmatize text\n",
    "format_sms = list(map(format_text, sms))\n",
    "lemmatize_sms = list(map(lemmatize_text, format_sms))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram\n",
    "In NLP, N-gram is a contiguous sequence of n words from a given sample of text. N-gram captures the context of the word by considering the words surrounding it in a sequence. N-gram can help identify relevant keywords or phrases that are indicative of a certain category, in this case, whether a text is a ham or spam.\n",
    "\n",
    "The number of phrases generated from a given corpus can be determined empirically to return a certain % of phrases over the total number of wrods. In this project, the number of bigrams is ~4% of unigrams and trigrams are ~1% of unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unigrams\n",
    "unigrams = set([token for sms in lemmatize_sms for token in sms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigram model\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# total no. of sms = 5573\n",
    "# total no. of unigrams = 8469\n",
    "\n",
    "# bigrams to represent ~4% of total unigrams\n",
    "\n",
    "bigram_model = Phrases(lemmatize_sms, min_count=5, threshold=3)\n",
    "\n",
    "# min_count=50, threshold=5         # returns 1 bigram\n",
    "# min_count=20, threshold=5         # returns 17 bigrams\n",
    "# min_count=10, threshold=5         # returns 90 bigrams\n",
    "# min_count=10, threshold=10        # returns 75 bigrams\n",
    "# min_count=5, threshold=5          # returns 287 bigrams\n",
    "# min_count=5, threshold=3          # returns 318 bigrams       [use this]\n",
    "\n",
    "# Apply bigram model to corpus\n",
    "corpus_w_bigrams = [Phraser(bigram_model)[sms] for sms in lemmatize_sms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigram model \n",
    "\n",
    "# trigrams to represent ~1% of total unigrams\n",
    "\n",
    "trigram_model = Phrases(corpus_w_bigrams, min_count=3, threshold=5)\n",
    "\n",
    "# min_count=5, threshold=5          # returns 23 trigrams\n",
    "# min_count=5, threshold=3          # returns 24 trigrams\n",
    "# min_count=3, threshold=3          # returns 102 trigrams\n",
    "# min_count=3, threshold=5          # returns 96 trigrams       [use this]\n",
    "\n",
    "# Apply trigram model to corpus_w_bigrams\n",
    "corpus_w_trigrams = [Phraser(trigram_model)[sms] for sms in corpus_w_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check corpus_w_bigrams & corpus_w_trigrams\n",
    "bigrams = set()\n",
    "for sms in corpus_w_bigrams:\n",
    "    for word in sms:\n",
    "        if len(word.split(\"_\")) == 2:\n",
    "            bigrams.add(word)\n",
    "            \n",
    "trigrams = set()\n",
    "for sms in corpus_w_trigrams:\n",
    "    for word in sms:\n",
    "        if len(word.split(\"_\")) == 3:\n",
    "            trigrams.add(word)\n",
    "            \n",
    "print(bigrams)            \n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to count occurences of token and calculate the percentage of total tokens\n",
    "def count_token(corpus):\n",
    "    \n",
    "    # Count the occurences of token in corpus\n",
    "    token_dict = {}\n",
    "    \n",
    "    for text in corpus:\n",
    "        for token in set(text):\n",
    "            if token in token_dict:\n",
    "                token_dict[token] += 1\n",
    "            elif token not in token_dict:\n",
    "                token_dict[token] = 1\n",
    "    \n",
    "    # Convert dictionary to dataframe for plot\n",
    "    token_df = (pd.DataFrame(token_dict, \n",
    "                             index=[0]).T\n",
    "                                       .reset_index()\n",
    "                                       .rename(columns={\"index\":\"token\", \n",
    "                                                        0:\"count\"}))\n",
    "    \n",
    "    # Calculate count percentage \n",
    "    total_token_count = token_df[\"count\"].sum()\n",
    "    token_df[\"count_perc\"] = token_df[\"count\"] / total_token_count * 100\n",
    "    \n",
    "    \n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine frequently occuring tokens\n",
    "df_unigrams_count = count_token(lemmatize_sms).sort_values(\"count_perc\", ascending=False)\n",
    "df_trigrams_count = count_token(corpus_w_trigrams).sort_values(\"count_perc\", ascending=False)\n",
    "\n",
    "# Top most frequently occuring tokens\n",
    "top_unigrams = df_unigrams_count.sort_values(\"count_perc\", ascending=False).head(10)\n",
    "top_trigrams = df_trigrams_count.sort_values(\"count_perc\", ascending=False).head(10)\n",
    "\n",
    "# Count of unigrams mostly higher than bigrams or trigrams\n",
    "# To identify top most frequently occuring trigrams\n",
    "only_trigrams_mask = df_trigrams_count[\"token\"].apply(lambda x: len(x.split(\"_\")) == 3)\n",
    "only_trigrams = df_trigrams_count[only_trigrams_mask]\n",
    "top_only_trigrams = only_trigrams.sort_values(\"count_perc\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group most frequently occuring words by ham or spam\n",
    "top_unigram_tokens = top_unigrams[\"token\"].values\n",
    "top_trigram_tokens = top_trigrams[\"token\"].values\n",
    "top_trigram_tokens_only = top_only_trigrams[\"token\"].values\n",
    "\n",
    "# Append list of unigrams and trigrams back into the data frame\n",
    "df[\"unigrams_list\"] = lemmatize_sms\n",
    "df[\"trigrams_list\"] = corpus_w_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of token occurence in each ham and spam group\n",
    "def search_token(top_token_list, column):\n",
    "    \n",
    "    tokens_df = pd.DataFrame()\n",
    "    \n",
    "    for token in top_token_list:\n",
    "        \n",
    "        # Returns True/False if token present in text\n",
    "        token_pres = df[column].apply(lambda x: token in x)\n",
    "        df[\"token_pres\"] = token_pres\n",
    "        \n",
    "        # Count occurences of token in ham and spam group\n",
    "        token_df = df.groupby(\"label\").agg(token_pres_count = (\"token_pres\", \"sum\"),\n",
    "                                           label_count = (\"token_pres\", \"count\"))\n",
    "        token_df[\"token_perc\"] = token_df[\"token_pres_count\"] / token_df[\"label_count\"] * 100\n",
    "        token_df[\"token\"] = [token] * 2\n",
    "        \n",
    "        token_df.reset_index(inplace=True)\n",
    "        \n",
    "        # Collect outputs into data frame\n",
    "        tokens_df = pd.concat([tokens_df, token_df], axis=0)\n",
    "    \n",
    "    return tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame for horizontal stacked barplot for unigram\n",
    "unigrams_stacked = search_token(top_unigram_tokens, \"unigrams_list\")\n",
    "unigrams_stacked.drop([\"token_pres_count\", \"label_count\"], axis=1, inplace=True)\n",
    "\n",
    "# Manually arrange barplot output\n",
    "token_seq = [\"like\", \"dont\", \"ill\", \"know\", \"ok\", \"call\", \"come\", \"im\", \"go\", \"get\"]\n",
    "unigrams_stacked[\"token\"] = pd.Categorical(unigrams_stacked[\"token\"],\n",
    "                                           token_seq)\n",
    "\n",
    "unigrams_stacked.sort_values(\"token\", inplace=True)\n",
    "\n",
    "weight_counts = {\n",
    "    \"ham\": unigrams_stacked[unigrams_stacked[\"label\"] == \"ham\"][\"token_perc\"].values,\n",
    "    \"spam\": unigrams_stacked[unigrams_stacked[\"label\"] == \"spam\"][\"token_perc\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame for horizontal stacked barplot for trigram\n",
    "trigrams_stacked = search_token(top_trigram_tokens_only, \"trigrams_list\")\n",
    "trigrams_stacked.drop([\"token_pres_count\", \"label_count\"], axis=1, inplace=True)\n",
    "\n",
    "# Manually arrange barplot output\n",
    "token_seq = ['log_onto_httpwwwurawinnercom', 'land_line_claim',\n",
    "             'urgent_please_call', 'valid_12hrs_150ppm',\n",
    "             'match_please_call', 'reply_call_08000930705',\n",
    "             'good_morning_dear', 'pls_send_message',\n",
    "             'happy_new_year', 'im_gon_na']\n",
    "trigrams_stacked[\"token\"] = pd.Categorical(trigrams_stacked[\"token\"],\n",
    "                                           token_seq)\n",
    "\n",
    "trigrams_stacked.sort_values(\"token\", inplace=True)\n",
    "\n",
    "weight_counts = {\n",
    "    \"ham\": trigrams_stacked[trigrams_stacked[\"label\"] == \"ham\"][\"token_perc\"].values,\n",
    "    \"spam\": trigrams_stacked[trigrams_stacked[\"label\"] == \"spam\"][\"token_perc\"].values,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure length of text for histogram\n",
    "df[\"unigrams_list\"] = lemmatize_sms\n",
    "df[\"unigrams_len\"] = df[\"unigrams_list\"].apply(lambda x: len(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "Codes for plots for the presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.ticker import FixedLocator, FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib reset\n",
    "matplotlib.rc_file_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham and spam count barplot\n",
    "count_df = df[\"label\"].value_counts()\n",
    "count_df = pd.DataFrame(count_df).reset_index()\n",
    "\n",
    "sns.set_theme(rc={\"figure.dpi\":300, 'savefig.dpi':300})   # adjust image resolution\n",
    "sns.set(rc={\"figure.facecolor\": \"#F8F8F8\",\n",
    "            \"figure.figsize\": (2, 4)})\n",
    "\n",
    "ax = sns.barplot(count_df, x=\"index\", y=\"label\", linewidth=0);\n",
    "\n",
    "# Setting fonts for tick labels\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), family=\"serif\");\n",
    "\n",
    "# Set title\n",
    "ax.set_title(\"ham & spam Count\", family=\"serif\", weight=\"bold\", size=14);\n",
    "\n",
    "# Remove axis labels\n",
    "ax.xaxis.label.set_visible(False)\n",
    "ax.yaxis.label.set_visible(False)\n",
    "\n",
    "# Removing spines\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_color(\"#CBCACA\")\n",
    "\n",
    "# Adding gridlines in the background\n",
    "ax.yaxis.grid(True, which=\"major\", linestyle=\"--\", alpha=0.05, color=\"#0C3578\")\n",
    "\n",
    "# Label sms label count directly\n",
    "for patch, ncount in zip(ax.patches, [\"4825\", \"747\"]):\n",
    "    ax.annotate(ncount, \n",
    "                (patch.get_x(), patch.get_height()),\n",
    "                ha = \"center\",\n",
    "                xytext=(patch.get_x() + patch.get_width()/2, patch.get_height()-300),\n",
    "                size=12,\n",
    "                color=\"#F8F8F8\",\n",
    "                family=\"serif\")\n",
    "\n",
    "# Set plot background color\n",
    "ax.set_facecolor(\"#F8F8F8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal stacked barplot of trigram proportions in ham and spam groups\n",
    "tokens = trigrams_stacked[trigrams_stacked[\"label\"] == \"spam\"][\"token\"].values\n",
    "\n",
    "sns.set_theme(rc={\"figure.dpi\":300, 'savefig.dpi':300})   # adjust image resolution\n",
    "sns.set(rc={\"figure.facecolor\": \"#F8F8F8\",\n",
    "            \"figure.figsize\": (4, 1.5)})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "left = np.zeros(10)\n",
    "\n",
    "for label, weight_count in weight_counts.items():\n",
    "    \n",
    "    if label == \"spam\":\n",
    "        color = \"#CB8962\"\n",
    "    else:\n",
    "        color = \"#5975A4\"\n",
    "    \n",
    "    p = ax.barh(tokens, weight_count, label=label, left=left, # type: ignore\n",
    "                linewidth=0, color=color)\n",
    "    left += weight_count # type: ignore\n",
    "    \n",
    "    \n",
    "# Setting fonts for tick labels\n",
    "ax.set_yticklabels(ax.get_yticklabels(), family=\"serif\", size=7)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), family=\"serif\", size=6)\n",
    "\n",
    "\n",
    "# Set title\n",
    "ax.set_title(\"Proportion of Trigram Based in ham and spam\",\n",
    "             family=\"serif\", size=10, weight=\"bold\")\n",
    "\n",
    "# Shift tick labels to top\n",
    "ax.tick_params(top=True, labeltop=True, bottom=False, labelbottom=False, color=\"#CBCACA\")\n",
    "ax.tick_params(axis=\"both\", which=\"major\", pad=0.1, length=2)\n",
    "\n",
    "# Set limit\n",
    "ax.set_xlim(0, 2)\n",
    "\n",
    "# Remove axis labels\n",
    "ax.xaxis.label.set_visible(False)\n",
    "ax.yaxis.label.set_visible(False)\n",
    "\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "# Removing spines\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_color(\"#CBCACA\")\n",
    "ax.spines[\"top\"].set_linewidth(0.8)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "\n",
    "# Adding gridlines in the background\n",
    "ax.yaxis.grid(False)\n",
    "ax.xaxis.grid(True, which=\"major\", linestyle=\"--\", alpha=0.05, color=\"#0C3578\")\n",
    "\n",
    "ax.set_facecolor(\"#F8F8F8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal stacked barplot of unigram proportions in ham and spam groups\n",
    "tokens = unigrams_stacked[unigrams_stacked[\"label\"] == \"spam\"][\"token\"].values\n",
    "\n",
    "sns.set_theme(rc={\"figure.dpi\":300, 'savefig.dpi':300})   # adjust image resolution\n",
    "sns.set(rc={\"figure.facecolor\": \"#F8F8F8\",\n",
    "            \"figure.figsize\": (4, 1.5)})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "left = np.zeros(10)\n",
    "\n",
    "for label, weight_count in weight_counts.items():\n",
    "    \n",
    "    if label == \"spam\":\n",
    "        color = \"#CB8962\"\n",
    "    else:\n",
    "        color = \"#5975A4\"\n",
    "    \n",
    "    p = ax.barh(tokens, weight_count, label=label, left=left, # type: ignore\n",
    "                linewidth=0, color=color)\n",
    "    left += weight_count # type: ignore\n",
    "    \n",
    "    \n",
    "# Setting fonts for tick labels\n",
    "ax.set_yticklabels(ax.get_yticklabels(), family=\"serif\", size=7)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), family=\"serif\", size=6)\n",
    "\n",
    "\n",
    "# Set title\n",
    "ax.set_title(\"Proportion of Unigram Based in ham and spam\",\n",
    "             family=\"serif\", size=10, weight=\"bold\")\n",
    "\n",
    "# Shift tick labels to top\n",
    "ax.tick_params(top=True, labeltop=True, bottom=False, labelbottom=False, color=\"#CBCACA\")\n",
    "ax.tick_params(axis=\"both\", which=\"major\", pad=0.1, length=2)\n",
    "\n",
    "# Set limit\n",
    "ax.set_xlim(0, 50)\n",
    "\n",
    "# Remove axis labels\n",
    "ax.xaxis.label.set_visible(False)\n",
    "ax.yaxis.label.set_visible(False)\n",
    "\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "# Removing spines\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_color(\"#CBCACA\")\n",
    "ax.spines[\"top\"].set_linewidth(0.8)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "\n",
    "# Adding gridlines in the background\n",
    "ax.yaxis.grid(False)\n",
    "ax.xaxis.grid(True, which=\"major\", linestyle=\"--\", alpha=0.05, color=\"#0C3578\")\n",
    "\n",
    "ax.set_facecolor(\"#F8F8F8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of length of text\n",
    "sns.set_theme(rc={\"figure.dpi\":300, 'savefig.dpi':300})   # adjust image resolution\n",
    "sns.set(rc={\"figure.facecolor\": \"#F8F8F8\",\n",
    "            \"figure.figsize\": (4, 3)})\n",
    "\n",
    "ax = sns.histplot(data=df, x=\"unigrams_len\", \n",
    "                  hue=\"label\", palette=[\"#5975A4\", \"#CB8962\"],\n",
    "                  kde=True, line_kws={\"linewidth\": 0.7},\n",
    "                  alpha=0.7, legend=False,\n",
    "                  linewidth=0.1);\n",
    "\n",
    "# Adjust grid lines\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, which=\"major\", linestyle=\"--\", alpha=0.1, linewidth=0.5, color=\"#0C3578\")\n",
    "\n",
    "# Edit axis tick labels\n",
    "ax.yaxis.set_major_locator(FixedLocator([150, 300, 450, 600]))\n",
    "ax.set_yticklabels(ax.get_yticklabels(), family=\"serif\", size=9)\n",
    "ax.set_xticklabels(ax.get_xticks(), family=\"serif\", size=10)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: int(x)))\n",
    "\n",
    "# Adjust titles\n",
    "ax.set_title(\"Histogram of Length of Text\", family=\"serif\", size=12, weight=\"bold\")\n",
    "ax.set_xlabel(\"Length of Text\", family=\"serif\", size=10)\n",
    "ax.yaxis.label.set_visible(False);\n",
    "\n",
    "# Adjust spines\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_color(\"#CBCACA\")\n",
    "ax.spines[\"bottom\"].set_linewidth(0.75)\n",
    "\n",
    "\n",
    "ax.set_facecolor(\"#F8F8F8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Vocabulary with TF-IDF\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure to evaluate the importance of a word in a document within a corpus. \n",
    "* Term Frequency measures how frequently the word occurs in the document.\n",
    "* Inverse Document Frequency measures how important a word based on how frequently it appears across multiple documents.\n",
    "\n",
    "TF-IDF for each token (n-gram) was calculated, and the mean of the values for each text was determined and plotted against the labels. From the boxplot, you can tell that the TF-IDF values are generally higher for ham messages than spam messages. It means that ham messages carry more important words than spam messages, possibly due to the repeatitive nature of the words that occur in spam messages.\n",
    "\n",
    "The TF-IDF means will be another feature added for the classification downstream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the TF-IDF using the gensim library\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Initialize a Dictionary that maps a token with an integer ID.\n",
    "vocab = Dictionary(corpus_w_trigrams)\n",
    "bow = [vocab.doc2bow(sms) for sms in corpus_w_trigrams]\n",
    "\n",
    "# Calculate the TF-IDF\n",
    "tfidf = TfidfModel(bow)\n",
    "corpus_tfidf = tfidf[bow]\n",
    "\n",
    "# Find TF-IDF mean for each text\n",
    "df[\"tfidf\"] = corpus_tfidf\n",
    "df[\"tfidf_mean\"] = df[\"tfidf\"].apply(lambda x: np.mean([i[1] for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for TF-IDF Mean\n",
    "sns.set_theme(rc={\"figure.dpi\":300, 'savefig.dpi':300})   # adjust image resolution\n",
    "sns.set(rc={\"figure.facecolor\": \"#F8F8F8\",\n",
    "            \"figure.figsize\": (3, 4)})\n",
    "\n",
    "ax = sns.boxplot(data=df, x=\"label\", y=\"tfidf_mean\");\n",
    "\n",
    "# Plot title \n",
    "ax.set_title(\"TF-IDF Mean Against Label\", family=\"serif\",\n",
    "             size=14, weight=\"bold\")\n",
    "\n",
    "# Remove axis labels\n",
    "ax.xaxis.label.set_visible(False)\n",
    "ax.set_ylabel(\"TF-IDF Mean\", family=\"serif\", size=10)\n",
    "\n",
    "# Set axis tick labels\n",
    "ax.set_yticklabels(ax.get_yticklabels(), family=\"serif\", size=9)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), family=\"serif\", size=10)\n",
    "\n",
    "# Removing spines\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_color(\"#CBCACA\")\n",
    "\n",
    "# Set grid\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, which=\"major\", linestyle=\"--\", \n",
    "              alpha=0.1, linewidth=0.5, color=\"#0C3578\")\n",
    "\n",
    "ax.set_facecolor(\"#F8F8F8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "Word embedding is a technique to represent words in a numerical form. It maps words into a dense vector that captures the semantic and syntactic information such as the words' context, its part of speech, and its relatedness to other words in a corpus.\n",
    "\n",
    "Word embedding is usually done using a large text corpus (Google News Database) to determine relationships and co-occurence frequency between words. The corpus has to be exceedingly large (100 billion words) for meaningful connection and interpretation of the words. The SMS corpus (8000 words) we have here is not large enough for a meaningful word embedding to be done, but we can still proceed with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding based only on sms corpus\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "word_vec = Word2Vec(corpus_w_trigrams,\n",
    "                    vector_size=100,        # size of your vector that represents the word\n",
    "                    window=3,               # number of words away from token\n",
    "                    min_count=1,            # min occurence of word\n",
    "                    epochs=5,\n",
    "                    seed=42)\n",
    "\n",
    "vectors = word_vec.wv                       # return an object KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Google News pretrained word2vec model\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "file_path = \"../dataset/nus_sms_corpus/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# Only unigrams in this language model\n",
    "gvectors = KeyedVectors.load_word2vec_format(file_path, binary=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "Supervised learning will be performed in two different ways:\n",
    "1. using TF-IDF sparse matrix\n",
    "2. word embedding dense vector\n",
    "\n",
    "The two features, Text Length and TF-IDF Mean, will be appended into the matrix to check if adding those features will result in better classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for ML training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode target labels (for some ML models)\n",
    "targets = np.where(df[\"label\"] == \"spam\", 1, 0)\n",
    "\n",
    "# convert list of trigrams back to text\n",
    "corpus_text = [text for text in map(lambda x: \" \".join(x), corpus_w_trigrams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes w sparse matrix\n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus_text,\n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=23)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_train_vect = vectorizer.fit_transform(x_train)\n",
    "x_test_vect = vectorizer.transform(x_test)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit_transform(x_train_vect);\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train_vect, y_train);\n",
    "\n",
    "y_pred_label = nb.predict(x_test_vect)\n",
    "\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving recall scores by prioritizing recall over precision\n",
    "y_pred_threshold = (nb.predict_proba(x_test_vect)[:,1] >= 0.3).astype(int)\n",
    "\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_threshold))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving recall scores by re-sampling\n",
    "from sklearn.utils import resample\n",
    "import ast\n",
    "\n",
    "resample_spam = resample(df[df[\"label\"]==\"spam\"], replace=True, n_samples=50, random_state=42)\n",
    "df_upsampled = pd.concat([df, resample_spam])\n",
    "\n",
    "#literal_w_trigrams = df_upsampled[\"trigrams_list\"].apply(ast.literal_eval)\n",
    "#corpus_w_trigrams = list(literal_w_trigrams.values)\n",
    "corpus_w_trigrams2 = df_upsampled[\"trigrams_list\"]\n",
    "\n",
    "# Re-compute the target and corpus_text\n",
    "targets = np.where(df_upsampled[\"label\"] == \"spam\", 1, 0)\n",
    "corpus_text = [text for text in map(lambda x: \" \".join(x), corpus_w_trigrams2)]\n",
    "\n",
    "# Re-train the model with upsampled dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus_text,\n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=23)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_train_vect = vectorizer.fit_transform(x_train)\n",
    "x_test_vect = vectorizer.transform(x_test)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit_transform(x_train_vect);\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train_vect, y_train);\n",
    "\n",
    "y_pred_label = nb.predict(x_test_vect)\n",
    "\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TfidfVectorizer (can try turning hyperparameter)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "tvec_x_train = tvec.fit_transform(x_train)\n",
    "tvec_x_test = tvec.transform(x_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(tvec_x_train, y_train)\n",
    "\n",
    "# scores = cross_val_score(nb, tvec_x_train, y_train, cv=5)\n",
    "y_pred_label = nb.predict(tvec_x_test)\n",
    "\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to dense vector using word vectors trained from SMS corpus\n",
    "\n",
    "vector_lists = []\n",
    "\n",
    "for sms in corpus_w_trigrams:\n",
    "    \n",
    "    vector_list = [vectors.word_vec(token) for token in sms if token in vectors]\n",
    "    \n",
    "    vector_avg = np.mean(vector_list, axis=0)\n",
    "    vector_avg = np.square(vector_avg)\n",
    "    \n",
    "    # After text formatting, sms is reduced to empty text as sms contains only stopwards\n",
    "    if not isinstance(vector_avg, np.ndarray):\n",
    "        vector_avg = np.array(np.zeros(100))\n",
    "    \n",
    "    vector_lists.append(vector_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to dense vector using word vectors trained from Google News Database\n",
    "\n",
    "gvector_lists = []\n",
    "\n",
    "for sms in lemmatize_sms:\n",
    "    \n",
    "    gvector_list = []\n",
    "    for token in sms:\n",
    "        if token in gvectors:\n",
    "            gvector_list.append(gvectors.word_vec(token))\n",
    "        else:\n",
    "            gvector_list.append(np.zeros(300))\n",
    "    \n",
    "    \n",
    "    gvector_avg = np.mean(gvector_list, axis=0)\n",
    "    \n",
    "    # After text formatting, sms is reduced to empty text as sms contains only stopwards\n",
    "    if not isinstance(gvector_avg, np.ndarray):\n",
    "        gvector_avg = np.zeros(300)\n",
    "    \n",
    "    gvector_lists.append(gvector_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending text length TF-IDF mean to data frame for ML training\n",
    "tfidf_mean = df[\"tfidf_mean\"].values\n",
    "tfidf_mean = np.nan_to_num(tfidf_mean, nan=0)\n",
    "\n",
    "unigrams_len = df[\"unigrams_len\"].values\n",
    "\n",
    "for i in range(len(vector_lists)):\n",
    "    vector_lists[i] = np.append(vector_lists[i], [tfidf_mean[i], unigrams_len[i]])\n",
    "\n",
    "for i in range(len(gvector_lists)):\n",
    "    gvector_lists[i] = np.append(gvector_lists[i], [tfidf_mean[i], unigrams_len[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale vector with MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "vector_scaled = scaler.fit_transform(vector_lists)\n",
    "gvector_scaled = scaler.fit_transform(gvector_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with dense vector\n",
    "x_train, x_test, y_train, y_test = train_test_split(gvector_scaled,\n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=23)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "y_pred_label = logreg.predict(x_test)\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifier with dense vector\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(gvector_scaled,\n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=23)\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "y_pred_label = svc.predict(x_test)\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost with sparse matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus_text,        # sparse matrix\n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=23)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_train_vect = vectorizer.fit_transform(x_train)\n",
    "x_test_vect = vectorizer.transform(x_test)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit_transform(x_train_vect);\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train_vect, y_train)\n",
    "\n",
    "y_pred_label = xgb.predict(x_test_vect)\n",
    "\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost with sparse matrix using gridsearchcv \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(vector_scaled,\n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=23)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "y_pred_label = xgb.predict(x_test)\n",
    "\n",
    "param_grid = {\"max_depth\": [3, 4, 5, 6, 7],\n",
    "              \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "              \"n_estimators\": [50, 100, 200, 300, 400, 500]}\n",
    "grid = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "y_pred_label = grid.predict(x_test)\n",
    "\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost with dense vector from sms corpus\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(vector_scaled,\n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=23)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "y_pred_label = xgb.predict(x_test)\n",
    "\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost with dense vector from google corpus\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(gvector_scaled,\n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=23)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "y_pred_label = xgb.predict(x_test)\n",
    "\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, y_pred_label))\n",
    "print(\"classification report: \\n\", classification_report(y_test, y_pred_label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "To determine if unsupervised learning can separate the texts into ham or spam based solely on the text in the messages. Target labels are unavailable in unsupervised learning. But we have the labels here, so we can still cross-check the predictions from the K-means clustering to see if unsupervised learning works here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import unsupervised learning libraries\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering model with dense vector (word embedding)\n",
    "km = KMeans(n_clusters=2,\n",
    "            init='k-means++',)\n",
    "km.fit(np.array(gvector_lists))     # with Google News word vector\n",
    "\n",
    "unsup_pred = km.predict(np.array(gvector_lists))\n",
    "\n",
    "# The original split of ham and spam is 4826 and 747 respectively.\n",
    "np.bincount(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confusion matrix: \\n\", confusion_matrix(unsup_pred, targets))\n",
    "print(\"classification report: \\n\", classification_report(unsup_pred, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering model with sparse matrix (TF-IDF)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Convert the text length and TF-IDF Mean features into a sparse matrix\n",
    "stfidf = sparse.csr_matrix(tfidf_mean).transpose()\n",
    "sunigrams_len = sparse.csr_matrix(unigrams_len).transpose()\n",
    "\n",
    "corpus_sparse = vectorizer.fit_transform(corpus_text)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_sparse = tfidf.fit_transform(corpus_sparse);\n",
    "\n",
    "# Stack the text length and TF-IDF Mean sparse matrix features\n",
    "tfidf_sparse2 = sparse.hstack((tfidf_sparse, stfidf, sunigrams_len))\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "km.fit(tfidf_sparse2)\n",
    "unsup_pred = km.predict(tfidf_sparse2)\n",
    "np.bincount(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confusion matrix: \\n\", confusion_matrix(unsup_pred, targets))\n",
    "print(\"classification report: \\n\", classification_report(unsup_pred, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting out the scatterplot of tfidf_mean and unigrams_len of the actual and predicted targets\n",
    "new_df = pd.DataFrame({\"tfidf_mean\": tfidf_mean,\n",
    "                       \"unigrams_len\": unigrams_len,\n",
    "                       \"pred_y\": unsup_pred,\n",
    "                       \"actual_y\": targets})\n",
    "\n",
    "sns.set_theme(rc={\"figure.dpi\":300, 'savefig.dpi':300})   # adjust image resolution\n",
    "sns.set(rc={\"figure.facecolor\": \"#F8F8F8\",\n",
    "            \"figure.figsize\": (12, 4)})\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "sns.scatterplot(new_df, x=\"tfidf_mean\", y=\"unigrams_len\", hue=\"pred_y\", ax=ax[0]);\n",
    "sns.scatterplot(new_df, x=\"tfidf_mean\", y=\"unigrams_len\", hue=\"actual_y\", ax=ax[1]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
